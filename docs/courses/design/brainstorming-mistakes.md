# Step 1.6 - What mistakes or misconceptions do you expect?

Write a list of common mistakes that you think students will make. These can be programming mistakes, conceptual misunderstandings, or simply examples of things that are unintuitive.


## Examples

From a course on generalized additive models:

- The difference between prediction intervals and confidence intervals.

From a course on business process mining:

- The structure of "process data" might be confusing to students new to the field, especially the distinction between events and activities. 

From a course on tree-based models:

- Not knowing which are the best hyperparameters to optimize for the different tree-based models.

## FAQs

### What types of mistakes do the students make?

There are several areas in which things can go wrong. The most obvious type of mistake is a coding error (since the student will get the answer wrong when they complete a coding exercise). More serious but harder to detect problems are failures to understand concepts, and misunderstandings about the meaning of jargon.


## Good ideas

### Categorize your responses

In order to think of things that can go wrong, it can be easier to narrow it down and deal with specific topics. What are the potential programming problems? What are the statistical problems? What domain-specific knowledge is hard to learn? Which bits or jargon will be unclear?

## Common problems and their solutions

### I can't think of anything

- To give you inspiration, take a look at [The R Inferno](http://www.burns-stat.com/pages/Tutor/R_inferno.pdf), a classic (by programming standards) text on things that can go wrong when learning R.
- Try explaining a concept or definition from the course out loud to someone with no data science background. Note the point at which they begin to look confused or their eyes glaze over.

## How will this be reviewed?

Your Curriculum Lead will discuss your responses to the brainstorming questions. They will not be formally reviewed (though they provide important context for reviewers).
