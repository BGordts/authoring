## Submission Correctness Tests

Automatically checking student submissions and providing meaningful feedback to tell them what they are doing wrong are central to the learning experience on DataCamp. The life and blood of the automated system that points out mistakes and guides students to the correct solution is the Submission Correctness Test, or SCT.

The SCT is a script of custom tests that accompanies every coding exercise. These custom tests have access to the code students submitted and the output and workspace they created with their code. For every taught language, there is an open-source library that provides a wide range of functions to verify these elements of a student submission. When the functions spot a mistake, they will automatically generate a meaningful feedback message.

The table below lists out all of the utlity packages used to write SCTs. If you're authoring R exercises you write your SCT in R. If you're building Python, SQL or Shell exercises, you write your SCT in Python. The documentation pages for each package list out all of its functions, with examples and best practices.

| Exercise Language | SCT language | GitHub | Documentation | Build Status  |
|:------------------|:-------------|:-------|:-------------:|:-------------:|
| R | R | [testwhat](https://github.com/datacamp/testwhat) | [link](https://datacamp.github.io/testwhat) | [![Build Status](https://travis-ci.org/datacamp/testwhat.svg?branch=master)](https://travis-ci.org/datacamp/testwhat) |
| Python | Python | [pythonwhat](https://github.com/datacamp/pythonwhat) | [link](http://pythonwhat.readthedocs.io/en/latest/) | [![Build Status](https://travis-ci.org/datacamp/pythonwhat.svg?branch=master)](https://travis-ci.org/datacamp/pythonwhat) |
| SQL | Python | [sqlwhat](https://github.com/datacamp/sqlwhat) | [link](http://sqlwhat.readthedocs.io/en/latest/) | [![Build Status](https://travis-ci.org/datacamp/sqlwhat.svg?branch=master)](https://travis-ci.org/datacamp/sqlwhat) |
| Shell | Python | [shellwhat](https://github.com/datacamp/shellwhat) | [link](https://shellwhat.readthedocs.io) | [![Build Status](https://travis-ci.org/datacamp/shellwhat.svg?branch=master)](https://travis-ci.org/datacamp/shellwhat) |

_In the remainder of this article, when `xwhat` is used, this means that the information applies to all of the SCT packages listed above._

### How it works

When a student starts an exercise on DataCamp, the coding backend:

- Starts a student coding process, and executes the `pre_execute_code` in this process. This code initializes the process with data, loads relevant packages,e tc, so that students can focus on the topic at hand.
- Starts a solution coding process at the same time, in which both the `pre_Exercise_code` and the `solution` are executed. This coding process represents the 'ideal final state' of an exercise.

When students click `Submit Answer`, the coding backend:

- Executes the submitted code in the student coding process and records any outputs or errors that are generated.
- Tells `xwhat` to check the submitted code, by calling the `test_exercise()` function that is available in all 4 of the SCT packages. Along with the SCT (the R/Python script with custom tests), the backend also passes the following information:
    + The student submission and the solution as text.
    + A reference to the student process and the solution process.
    + The output and errors that were generated when executing the student code.

  If there is a failing test in the SCT, `xwhat` marks the submitted code as incorrect and automatically generates a feedback message. If all tests pass, `xwhat` marks the submitted code as correct, and generates a success message. This information is relayed back to the coding backend.
- Bundles the code output and the correctness information, so it can be shown in the learning interface.

### Example

To understand how SCTs affect the student's experience, consider the markdown source for an R exercise about variable assignment:

    ## Create a variable

    ```yaml
    type: NormalExercise
    ```

    In this exercise, you'll assign your first variable.

    `@instructions`
    Create a variable `m`, equal to 5.

    `@sample_code`
    ```{r}
    # Create m

    ```

    `@solution`
    ```{r}
    # Create m
    m <- 5
    ```

    `@sct`
    ```{r}
    ex() %>% check_object("m") %>% check_equal()
    success_msg("Well done!")
    ```

- Student submits `a <- 4`
    + Feedback box appears: "Did you define the variable `m` without errors?".
    + This message is generated by `check_object()`, that checks if `m` was defined in the student coding session.
- Student submits `m <- 4` (correct variable name, incorrect value)
    + Feedback box appears: "The contents of the variable `m` aren't correct.".
    + This message was generated by `check_equal()`, that compares the value of `m` in the student coding session with the value of `m` in the solution coding session.
    + Notice that there was no need to repeat the value `5` in the SCT; `testwhat` inferred it.
- Student submits `m <- 5` (correct answer)
    + All checks pass, and the message "Well done!" is shown, as specified in `success_msg()`.

### Concept of state

As you could see in the example above, `testwhat` uses [`magrittr`'s pipe operator](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) to 'chain together' SCT functions. Every chain starts with the `ex()` function call, which holds the exercise state. This exercise state contains the pieces of information mentioned ealier, such as the student submission and solution as text, a reference to the student and solution process, etc. As SCT functions are chained together, the exercise state is copied and adapted to zoom in on particular parts of the code. Consider the following example:

```R
# Solution to check
x <- TRUE
if (x) {
  print("x is TRUE!")
}

# SCT
if_state <- ex() %>% check_if_else()
if_state %>% check_cond() %>% check_code("x")
if_state %>% check_if() %>% check_function("print") %>% check_arg("x") %>% check_equal()

# Same SCT, less verbose
if_state <- ex() %>% check_if_else() %>% {
    check_cond(.) %>% check_code("x")
    check_if(.) %>% check_function("print") %>% check_arg("x") %>% check_equal()
}
```

`check_if_else()` will check whether an `if` statement was coded, and will afterwards 'zoom in' on the if loop only. `check_cond()` will consequently zoom in on the condition part of the `if` statement. Hence, `check_code()` is will only look for the occurence of `"x"` inside the condition of the `if` statement. Similarly, `check_if()` starts from the `if` statement, and zooms in on the body of the `if` statement, after which `check_function()` will only look for the `print` call inside the body, ignoring anything outside of it.

The other Python-based SCT libraries also feature the concept of state, the concept of state works the same way, but has slightly different syntax because of language internals. The root state is stored in `Ex()`, `.` is used for chaining, and `multi()` for branching of into multiple chains of tests.

The Python equivalent of the variable assignment example would look as follows:

```Python
# Solution to check
m = 5

# SCT
Ex().check_object('m').has_equal_value()
```

The Python equivalent of the `if` statement example would look as follows:

```Python
# Solution to check
x = True
if x: print('x is TRUE!')

# SCT
Ex().check_if_exp(0).check_body().test_student_typed('x'),
Ex().check_if_exp(0).check_test().check_function('print').check_args('value').has_equal_value()()

# SCT (less verbose, more performant)
Ex().check_if_exp(0).multi(
        check_body().test_student_typed('x'),
        check_test().check_function('print').check_args('value').has_equal_value()()
        )
```

Similar to how it is done in R, every `.` means 'stepping into a sub state', that only looks at parts of the code and parts of the student and solution processes when running the following checks.